{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Modeling"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:37:08.454957Z","iopub.status.busy":"2023-11-03T22:37:08.454571Z","iopub.status.idle":"2023-11-03T22:37:08.850317Z","shell.execute_reply":"2023-11-03T22:37:08.849246Z","shell.execute_reply.started":"2023-11-03T22:37:08.454916Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","import numpy as np\n","import math\n","from torch.autograd import Variable\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:37:08.852779Z","iopub.status.busy":"2023-11-03T22:37:08.852331Z","iopub.status.idle":"2023-11-03T22:37:08.857548Z","shell.execute_reply":"2023-11-03T22:37:08.856296Z","shell.execute_reply.started":"2023-11-03T22:37:08.852747Z"},"trusted":true},"outputs":[],"source":["train_path = r'./train_data.csv'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:37:08.859213Z","iopub.status.busy":"2023-11-03T22:37:08.858908Z","iopub.status.idle":"2023-11-03T22:37:30.089479Z","shell.execute_reply":"2023-11-03T22:37:30.088554Z","shell.execute_reply.started":"2023-11-03T22:37:08.859186Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(train_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:37:30.092751Z","iopub.status.busy":"2023-11-03T22:37:30.092415Z","iopub.status.idle":"2023-11-03T22:37:30.133544Z","shell.execute_reply":"2023-11-03T22:37:30.132439Z","shell.execute_reply.started":"2023-11-03T22:37:30.092722Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sequence_id</th>\n","      <th>sequence</th>\n","      <th>experiment_type</th>\n","      <th>dataset_name</th>\n","      <th>reactivity_0001</th>\n","      <th>reactivity_0002</th>\n","      <th>reactivity_0003</th>\n","      <th>reactivity_0004</th>\n","      <th>reactivity_0005</th>\n","      <th>reactivity_0006</th>\n","      <th>...</th>\n","      <th>reactivity_error_0197</th>\n","      <th>reactivity_error_0198</th>\n","      <th>reactivity_error_0199</th>\n","      <th>reactivity_error_0200</th>\n","      <th>reactivity_error_0201</th>\n","      <th>reactivity_error_0202</th>\n","      <th>reactivity_error_0203</th>\n","      <th>reactivity_error_0204</th>\n","      <th>reactivity_error_0205</th>\n","      <th>reactivity_error_0206</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000d87cab97</td>\n","      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n","      <td>2A3_MaP</td>\n","      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_2A3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0000d87cab97</td>\n","      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n","      <td>DMS_MaP</td>\n","      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_DMS</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0001ca9d21b0</td>\n","      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n","      <td>2A3_MaP</td>\n","      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001ca9d21b0</td>\n","      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n","      <td>DMS_MaP</td>\n","      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00021f968267</td>\n","      <td>GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...</td>\n","      <td>2A3_MaP</td>\n","      <td>DasLabBigLib_OneMil_Replicates_from_previous_l...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 416 columns</p>\n","</div>"],"text/plain":["    sequence_id                                           sequence  \\\n","0  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n","1  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n","2  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n","3  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n","4  00021f968267  GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...   \n","\n","  experiment_type                                       dataset_name  \\\n","0         2A3_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_2A3   \n","1         DMS_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_DMS   \n","2         2A3_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3   \n","3         DMS_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS   \n","4         2A3_MaP  DasLabBigLib_OneMil_Replicates_from_previous_l...   \n","\n","   reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n","0              NaN              NaN              NaN              NaN   \n","1              NaN              NaN              NaN              NaN   \n","2              NaN              NaN              NaN              NaN   \n","3              NaN              NaN              NaN              NaN   \n","4              NaN              NaN              NaN              NaN   \n","\n","   reactivity_0005  reactivity_0006  ...  reactivity_error_0197  \\\n","0              NaN              NaN  ...                    NaN   \n","1              NaN              NaN  ...                    NaN   \n","2              NaN              NaN  ...                    NaN   \n","3              NaN              NaN  ...                    NaN   \n","4              NaN              NaN  ...                    NaN   \n","\n","   reactivity_error_0198  reactivity_error_0199  reactivity_error_0200  \\\n","0                    NaN                    NaN                    NaN   \n","1                    NaN                    NaN                    NaN   \n","2                    NaN                    NaN                    NaN   \n","3                    NaN                    NaN                    NaN   \n","4                    NaN                    NaN                    NaN   \n","\n","   reactivity_error_0201  reactivity_error_0202  reactivity_error_0203  \\\n","0                    NaN                    NaN                    NaN   \n","1                    NaN                    NaN                    NaN   \n","2                    NaN                    NaN                    NaN   \n","3                    NaN                    NaN                    NaN   \n","4                    NaN                    NaN                    NaN   \n","\n","   reactivity_error_0204  reactivity_error_0205  reactivity_error_0206  \n","0                    NaN                    NaN                    NaN  \n","1                    NaN                    NaN                    NaN  \n","2                    NaN                    NaN                    NaN  \n","3                    NaN                    NaN                    NaN  \n","4                    NaN                    NaN                    NaN  \n","\n","[5 rows x 416 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:37:30.135177Z","iopub.status.busy":"2023-11-03T22:37:30.134835Z","iopub.status.idle":"2023-11-03T22:37:31.802787Z","shell.execute_reply":"2023-11-03T22:37:31.801892Z","shell.execute_reply.started":"2023-11-03T22:37:30.135149Z"},"trusted":true},"outputs":[],"source":["train, val = train_test_split(train_df, test_size=0.2, random_state=283)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:37:35.417192Z","iopub.status.busy":"2023-11-03T22:37:35.416719Z","iopub.status.idle":"2023-11-03T22:37:35.421714Z","shell.execute_reply":"2023-11-03T22:37:35.420593Z","shell.execute_reply.started":"2023-11-03T22:37:35.417162Z"},"trusted":true},"outputs":[],"source":["max_seq_length = 407\n","nucleotides = 'ACGU'"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:37:35.423633Z","iopub.status.busy":"2023-11-03T22:37:35.423257Z","iopub.status.idle":"2023-11-03T22:37:35.435059Z","shell.execute_reply":"2023-11-03T22:37:35.433990Z","shell.execute_reply.started":"2023-11-03T22:37:35.423595Z"},"trusted":true},"outputs":[],"source":["def str_to_seq(s):\n","    mapping = {nucleotide: idx for idx, nucleotide in enumerate(nucleotides)}\n","    return [mapping[c] for c in s]\n","\n","\n","def train_test_split(\n","        df: pd.DataFrame,\n","        test_size: float,\n","        random_state: int\n","):\n","    n = len(df)\n","    rng = np.random.default_rng(seed=random_state)\n","    idx = rng.permutation(n)\n","    split = math.floor(n * test_size)\n","    return df.iloc[idx[split:]], df.iloc[idx[:split]]\n","\n","def extract_input_seq(df: pd.DataFrame, idx):\n","    input_seq = df['sequence'].iloc[idx]\n","    input_seq = str_to_seq(input_seq)\n","    input_seq = torch.LongTensor(input_seq)\n","    input_seq = F.one_hot(input_seq, num_classes=len(nucleotides))\n","    input_seq = input_seq.float()\n","    input_seq = F.pad(input_seq, pad=(0, 0, 0, max_seq_length - input_seq.size(0)))\n","\n","    return input_seq\n","\n","\n","def extract_label_seq(df: pd.DataFrame, label_idx, idx):\n","    label_seq = df.iloc[idx, label_idx]\n","    label_seq = torch.FloatTensor(label_seq.to_numpy(dtype=float))\n","    label_seq = torch.nan_to_num(label_seq)\n","    label_seq = F.pad(label_seq, pad=(0, max_seq_length - label_seq.size(0)))\n","\n","    return label_seq\n","\n","\n","class RNADataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","        self.label_idx = [idx for idx, column in enumerate(self.df.columns) if\n","                          not column.startswith('reactivity_error') and column.startswith('reactivity')]\n","        self.err_idx = [idx for idx, column in enumerate(self.df.columns) if\n","                        column.startswith('reactivity_error')]\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        input_seq = extract_input_seq(self.df, idx)\n","        label_seq = extract_label_seq(self.df, self.label_idx, idx)\n","        err_seq = extract_label_seq(self.df, self.err_idx, idx)\n","        return input_seq, label_seq, len(self.df['sequence'].iloc[idx]), int(self.df['experiment_type'].iloc[idx] == 'DMS_MaP')\n","\n","\n","class RNAPredictDataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        input_seq = extract_input_seq(self.df, idx)\n","\n","        return input_seq #, len(self.df['sequence'].iloc[idx])\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:37:35.436664Z","iopub.status.busy":"2023-11-03T22:37:35.436304Z","iopub.status.idle":"2023-11-03T22:37:35.443888Z","shell.execute_reply":"2023-11-03T22:37:35.442871Z","shell.execute_reply.started":"2023-11-03T22:37:35.436590Z"},"trusted":true},"outputs":[],"source":["batch_size = 128"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:37:35.447966Z","iopub.status.busy":"2023-11-03T22:37:35.447618Z","iopub.status.idle":"2023-11-03T22:37:35.454509Z","shell.execute_reply":"2023-11-03T22:37:35.453596Z","shell.execute_reply.started":"2023-11-03T22:37:35.447940Z"},"trusted":true},"outputs":[],"source":["train_loader = DataLoader(RNADataset(train), batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(RNADataset(val), batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Define and Train Model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:37:35.780296Z","iopub.status.busy":"2023-11-03T22:37:35.779978Z","iopub.status.idle":"2023-11-03T22:37:35.859269Z","shell.execute_reply":"2023-11-03T22:37:35.858016Z","shell.execute_reply.started":"2023-11-03T22:37:35.780269Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:41:26.154842Z","iopub.status.busy":"2023-11-03T22:41:26.154433Z","iopub.status.idle":"2023-11-03T22:41:26.162987Z","shell.execute_reply":"2023-11-03T22:41:26.161926Z","shell.execute_reply.started":"2023-11-03T22:41:26.154811Z"},"trusted":true},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \"Implement the PE function.\"\n","\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n","        return self.dropout(x)\n","\n","\n","class RNAModel(nn.Module):\n","    def __init__(self, embed_dim, d_model, nhead, num_layers, dropout):\n","        super().__init__()\n","\n","        self.conv = nn.Conv1d(embed_dim, d_model, kernel_size=3, padding=1)\n","        self.pe = PositionalEncoding(d_model=d_model, dropout=dropout)\n","        self.te = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True),\n","            num_layers=num_layers\n","        )\n","        self.linear = nn.Linear(d_model, 2)\n","\n","    def forward(self, x):\n","        x = self.conv(x.transpose(-1, -2)).transpose(-1, -2)\n","        x = self.pe(x)\n","        x = self.te(x)\n","        x = self.linear(x)\n","\n","        return x.squeeze()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:41:27.656120Z","iopub.status.busy":"2023-11-03T22:41:27.655341Z","iopub.status.idle":"2023-11-03T22:41:27.692969Z","shell.execute_reply":"2023-11-03T22:41:27.691863Z","shell.execute_reply.started":"2023-11-03T22:41:27.656086Z"},"trusted":true},"outputs":[],"source":["model = RNAModel(\n","    embed_dim=len(nucleotides),\n","    d_model=128,\n","    nhead=4,\n","    num_layers=4,\n","    dropout=.01\n",").to(device)\n","\n","criterion = nn.MSELoss()\n","mae = nn.L1Loss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n","num_epochs = 5"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-03T22:42:08.428396Z","iopub.status.busy":"2023-11-03T22:42:08.427585Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":[" 20%|██        | 1/5 [07:08<28:35, 428.95s/it]"]},{"name":"stdout","output_type":"stream","text":["epoch 0, train loss : 0.15595159970190936, valid loss : 0.12590118477275392, valid mae : 0.15653921639785728\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 2/5 [14:17<21:26, 428.91s/it]"]},{"name":"stdout","output_type":"stream","text":["epoch 1, train loss : 0.12322047734671522, valid loss : 0.11751575883075895, valid mae : 0.14972028674989696\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|██████    | 3/5 [21:33<14:23, 431.88s/it]"]},{"name":"stdout","output_type":"stream","text":["epoch 2, train loss : 0.11577148947597762, valid loss : 0.11190346149274501, valid mae : 0.14847095789397036\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████  | 4/5 [28:47<07:12, 432.63s/it]"]},{"name":"stdout","output_type":"stream","text":["epoch 3, train loss : 0.11125850759795934, valid loss : 0.10853968367471604, valid mae : 0.1375571650138068\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5/5 [36:01<00:00, 432.28s/it]"]},{"name":"stdout","output_type":"stream","text":["epoch 4, train loss : 0.10879031719146978, valid loss : 0.10599314082030083, valid mae : 0.13851197617133265\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["loss_train = np.zeros(num_epochs)\n","loss_valid = np.zeros(num_epochs)\n","loss_mae_valid = np.zeros(num_epochs)\n","\n","for epoch in tqdm(range(num_epochs)):\n","    train_size = 0\n","    valid_size = 0\n","\n","    # train for one epoch\n","    model.train()\n","    for inputs, labels, seq_lengths, is_dmp in  train_loader:\n","        n_batch_train = len(inputs)\n","        inputs, labels, is_dmp = inputs.to(device), labels.to(device), is_dmp.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        outputs = outputs[torch.arange(outputs.size(0)), :, is_dmp]\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        loss_train[epoch] += loss.item() * n_batch_train\n","        train_size += n_batch_train\n","    \n","    # validation\n","    with torch.no_grad():\n","        model.eval()\n","        for inputs, labels, seq_lengths, is_dmp in val_loader:\n","            n_batch_valid = len(inputs)\n","            inputs, labels, is_dmp = inputs.to(device), labels.to(device), is_dmp.to(device)\n","            outputs = model(inputs)\n","            outputs = outputs[torch.arange(outputs.size(0)), :, is_dmp]\n","            loss = criterion(outputs, labels)\n","            loss_valid[epoch] += loss.item() * n_batch_valid\n","            mae_loss = mae(outputs, labels)\n","            loss_mae_valid[epoch] += mae_loss.item() * n_batch_valid\n","            valid_size += n_batch_valid\n","    \n","    loss_train[epoch] /= train_size\n","    loss_valid[epoch] /= valid_size\n","    loss_mae_valid[epoch] /= valid_size\n","    print(\"epoch {}, train loss : {}, valid loss : {}, valid mae : {}\".format(epoch, loss_train[epoch], loss_valid[epoch], loss_mae_valid[epoch]))"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["test_df = pd.read_csv('test_sequences.csv')\n","seq_len = [len(seq) for seq in test_df.sequence.to_numpy()]\n","testloader = DataLoader(RNAPredictDataset(test_df), batch_size=128, shuffle=False)"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([128, 407, 4])\n"]}],"source":["for i in testloader:\n","    print(i.shape)\n","    break"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10499/10499 [15:39<00:00, 11.17it/s]\n"]}],"source":["predictions = []\n","with torch.no_grad():\n","    model.eval()\n","    for batch in tqdm(testloader):\n","        batch = batch.to(device)\n","        outputs = model(batch)\n","        predictions.append(outputs.cpu().numpy())"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10499/10499 [00:00<00:00, 19995.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[-0.0018136  0.0070217]\n","[-0.0018136  0.0070217]\n","(269496671, 2)\n"]}],"source":["# np.append too slow\n","\n","modified_preds = []\n","i = 0\n","for batch_preds in tqdm(predictions):\n","    for j, pred in enumerate(batch_preds):\n","        # print(pred.shape)\n","        # print(pred[:seq_len[i+j]].shape)\n","        modified_preds.append(pred[:seq_len[i+j]]) \n","    i += j + 1\n","print(modified_preds[0][0])\n","modified_preds = np.concatenate(modified_preds)\n","print(modified_preds[0])\n","print(modified_preds.shape)"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["df = pd.DataFrame({\n","    'id': np.arange(modified_preds.shape[0]),\n","    'reactivity_DMS_MaP': modified_preds[:, 1],\n","    'reactivity_2A3_MaP': modified_preds[:, 0]\n","})"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["df.to_csv('submission.csv.gz', index=False, compression='gzip')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
